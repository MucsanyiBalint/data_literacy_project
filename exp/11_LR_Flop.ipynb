{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35ef4fb-3a98-4cf1-b4e2-1b87654cf20e",
   "metadata": {},
   "source": [
    "### Selecting columns, viualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5b3334-f8fc-4bc0-ba11-a29eff4269d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43f9484-66ab-455b-ad5b-e52f5971e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Balint\\miniconda3\\envs\\d2l\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (5,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../dat/data_clean_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d756f1c-509d-4a86-83fd-de9c7b14aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 273543 entries, 0 to 273542\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   tconst                       273543 non-null  object \n",
      " 1   titleType                    273543 non-null  object \n",
      " 2   primaryTitle                 273543 non-null  object \n",
      " 3   originalTitle                273543 non-null  object \n",
      " 4   isAdult                      273543 non-null  int64  \n",
      " 5   startYear                    273543 non-null  object \n",
      " 6   endYear                      273543 non-null  object \n",
      " 7   runtimeMinutes               273543 non-null  object \n",
      " 8   genres                       273543 non-null  object \n",
      " 9   averageRating                273543 non-null  float64\n",
      " 10  numVotes                     273543 non-null  int64  \n",
      " 11  Budget                       49881 non-null   float64\n",
      " 12  Gross US & Canada            18982 non-null   float64\n",
      " 13  Opening weekend US & Canada  16112 non-null   float64\n",
      " 14  Gross worldwide              45004 non-null   float64\n",
      " 15  Rating                       82925 non-null   object \n",
      " 16  Critic reviews               89561 non-null   object \n",
      " 17  User reviews                 114165 non-null  object \n",
      " 18  directors                    273543 non-null  object \n",
      " 19  writers                      273543 non-null  object \n",
      "dtypes: float64(5), int64(2), object(13)\n",
      "memory usage: 41.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78fc93f-150c-446e-81db-3804a9cffcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    }
   ],
   "source": [
    "data[\"Critic reviews\"] = data[\"Critic reviews\"].fillna(0)\n",
    "data[\"User reviews\"] = data[\"User reviews\"].fillna(0)\n",
    "\n",
    "data[\"isAdult2\"] = data.apply(lambda row: int(\"Adult\" in row[\"genres\"]), axis=1)\n",
    "print(len(data[data[\"isAdult\"] != data[\"isAdult2\"]]))  # We use the one based on the genres\n",
    "\n",
    "# tconst was only required for joins\n",
    "# titleType is only films for us, we filtered them\n",
    "# we do not use the titles as predictors\n",
    "# endYear is None for all films\n",
    "# isAdult will be added back in a consistent format later on\n",
    "\n",
    "# We drop writers and directors. These are interesting features,\n",
    "# but having them as binary columns would be infeasible.\n",
    "data = data.drop(columns=[\n",
    "    \"tconst\", \"titleType\", \"primaryTitle\", \"originalTitle\", \"endYear\",\n",
    "    \"isAdult\", \"isAdult2\", \"Gross US & Canada\", \"Opening weekend US & Canada\",\n",
    "    \"writers\", \"directors\"])\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1953459-68c5-437c-9da7-4bd1c8390acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Horror', 'Music', 'Drama', 'Biography', 'Action', 'History', 'Thriller', 'Mystery', 'Sci-Fi', 'Musical', 'Crime', 'Fantasy', 'News', 'Film-Noir', 'Adventure', 'Family', 'Animation', 'Western', 'Documentary', 'Romance', 'Comedy', 'Sport', 'War'}\n"
     ]
    }
   ],
   "source": [
    "genre_list = data[\"genres\"].unique().tolist()\n",
    "for i, entry in enumerate(genre_list):\n",
    "    genre_list[i] = entry.split(\",\")\n",
    "\n",
    "genre_set = set(itertools.chain(*genre_list))\n",
    "print(genre_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278fa605-7d45-41dc-9c57-92f001a7a30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horror Music Drama Action Thriller Mystery Sci-Fi Musical Crime Fantasy Adventure Family Animation Documentary Romance Comedy Sport War "
     ]
    }
   ],
   "source": [
    "# News - History - Biography - Documentary --> Documentary\n",
    "# Film-Noir - Crime --> Crime\n",
    "# Western - Action --> Action\n",
    "genre_set.difference_update([\"News\", \"History\", \"Biography\", \"Film-Noir\", \"Western\"])\n",
    "transformation_dict = {\n",
    "    \"Documentary\":  [\"News\", \"History\", \"Biography\", \"Documentary\"],\n",
    "    \"Crime\": [\"Film-Noir\", \"Crime\"],\n",
    "    \"Action\": [\"Western\", \"Action\"]\n",
    "}\n",
    "for genre in genre_set:\n",
    "    print(genre, end=\" \")\n",
    "    if genre not in transformation_dict:\n",
    "        transformation_dict[genre] = [genre]\n",
    "    data[f\"is{genre}\"] = data.apply(lambda row: int(any(g in row[\"genres\"] for g in transformation_dict[genre])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae03aed-0277-408e-90ab-f61534aaa724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horror 4.348486574759789 % -  1145\n",
      "Music 1.7659792639854164 % -  465\n",
      "Drama 21.920929702631877 % -  5772\n",
      "Action 9.843910219892901 % -  2592\n",
      "Thriller 6.63096730090008 % -  1746\n",
      "Mystery 3.4901826744141884 % -  919\n",
      "Sci-Fi 2.529338042611371 % -  666\n",
      "Musical 0.5089058524173028 % -  134\n",
      "Crime 7.569025103490183 % -  1993\n",
      "Fantasy 2.8179712126390943 % -  742\n",
      "Adventure 6.7107212031445815 % -  1767\n",
      "Family 2.2938741407466483 % -  604\n",
      "Animation 1.993847556112567 % -  525\n",
      "Documentary 4.564961452280582 % -  1202\n",
      "Romance 6.737305837226083 % -  1774\n",
      "Comedy 14.488625574417988 % -  3815\n",
      "Sport 0.9190687782461737 % -  242\n",
      "War 0.8658995100831719 % -  228\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for genre in genre_set:\n",
    "    results.append(data[f\"is{genre}\"].sum())\n",
    "\n",
    "sum_results = sum(results)\n",
    "for genre in genre_set:\n",
    "    print(genre, data[f\"is{genre}\"].sum() / sum_results * 100, \"% - \", data[f\"is{genre}\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f98425f-60ec-4147-92cf-16760d26322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genres are added as binary predictors, thus the genres column is no longer used.\n",
    "data = data.drop(columns=[\"genres\"])  # \"isMusical\", \"isFilm-Noir\", \"isNews\", \"isSport\", \"genres\"])\n",
    "\n",
    "def unrated_to_not_rated(row):\n",
    "    if row[\"Rating\"] == \"Unrated\":\n",
    "        return \"Not Rated\"\n",
    "    else:\n",
    "        return row[\"Rating\"]\n",
    "\n",
    "data[\"Rating\"] = data.apply(unrated_to_not_rated, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88325f4-9d05-44c2-8608-46d92f940cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[f\"isRated\"] = data.apply(lambda row: int(row[\"Rating\"] != \"Not Rated\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babc7280-9c11-4336-ba64-5db77fd99196",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce8e91cf-33bb-4d49-8f99-8a6fe2d090fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unknowns(row, column):\n",
    "    if row[column] == \"\\\\N\":\n",
    "        return None\n",
    "    else:\n",
    "        return row[column]\n",
    "\n",
    "def clean_reviews(row, column):\n",
    "    if isinstance(row[column], str) and \"K\" in row[column]:\n",
    "        # print(row[column], end=\" -> \")\n",
    "        if \".\" in row[column]:\n",
    "            # print(int(row[column][:-3]) * 1000 + int(row[column][-2]) * 100)\n",
    "            return int(row[column][:-3]) * 1000 + int(row[column][-2]) * 100\n",
    "        else:\n",
    "            # print(int(row[column][:-1]) * 1000)\n",
    "            return int(row[column][:-1]) * 1000\n",
    "    else:\n",
    "        return row[column]\n",
    "\n",
    "# Just an example of problematic data types\n",
    "# print(\"Problematic form\")\n",
    "# print(data.startYear.unique())\n",
    "# print(data.runtimeMinutes.unique())\n",
    "# print(data[\"User reviews\"].unique())\n",
    "# print(data[\"Critic reviews\"].unique())\n",
    "\n",
    "data[\"startYear\"] = data.apply(lambda row: clean_unknowns(row, \"startYear\"), axis=1)\n",
    "data[\"runtimeMinutes\"] = data.apply(lambda row: clean_unknowns(row, \"runtimeMinutes\"), axis=1)\n",
    "data[\"User reviews\"] = data.apply(lambda row: clean_reviews(row, \"User reviews\"), axis=1)\n",
    "data[\"Critic reviews\"] = data.apply(lambda row: clean_reviews(row, \"Critic reviews\"), axis=1)\n",
    "\n",
    "for column in [\"startYear\", \"runtimeMinutes\", \"User reviews\", \"Critic reviews\"]:\n",
    "    data[column] = pd.to_numeric(data[column])\n",
    "\n",
    "# print(\"Resolved form\")\n",
    "# print(data.startYear.unique())\n",
    "# print(data.runtimeMinutes.unique())\n",
    "# print(data[\"User reviews\"].unique())\n",
    "# print(data[\"Critic reviews\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ef5878-ecf4-43ec-960b-74d1ed561e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data[\"isFlop\"] = (data[\"Gross worldwide\"] < data[\"Budget\"]).astype(float)\n",
    "data = data.drop(columns=[\"Gross worldwide\", \"Budget\"])\n",
    "filtered = data.dropna()  # data.dropna()\n",
    "print(len(filtered))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b6f18ad-47b5-4a06-a873-409a23e0e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.random.choice(len(filtered), replace=False, size=int(len(filtered) / 10))\n",
    "test_set = filtered.iloc[test_indices]\n",
    "test_set, test_targets = test_set.drop(\"isFlop\", axis=1).to_numpy(), test_set[\"isFlop\"].to_numpy()\n",
    "train_set = filtered.iloc[~test_indices]\n",
    "train_set, train_targets = train_set.drop(\"isFlop\", axis=1).to_numpy(), train_set[\"isFlop\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058ca107-e27e-4861-9f4b-427560e7ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10714 entries, 1031 to 273540\n",
      "Data columns (total 26 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   startYear       10714 non-null  int64  \n",
      " 1   runtimeMinutes  10714 non-null  float64\n",
      " 2   averageRating   10714 non-null  float64\n",
      " 3   numVotes        10714 non-null  int64  \n",
      " 4   Critic reviews  10714 non-null  float64\n",
      " 5   User reviews    10714 non-null  float64\n",
      " 6   isHorror        10714 non-null  int64  \n",
      " 7   isMusic         10714 non-null  int64  \n",
      " 8   isDrama         10714 non-null  int64  \n",
      " 9   isAction        10714 non-null  int64  \n",
      " 10  isThriller      10714 non-null  int64  \n",
      " 11  isMystery       10714 non-null  int64  \n",
      " 12  isSci-Fi        10714 non-null  int64  \n",
      " 13  isMusical       10714 non-null  int64  \n",
      " 14  isCrime         10714 non-null  int64  \n",
      " 15  isFantasy       10714 non-null  int64  \n",
      " 16  isAdventure     10714 non-null  int64  \n",
      " 17  isFamily        10714 non-null  int64  \n",
      " 18  isAnimation     10714 non-null  int64  \n",
      " 19  isDocumentary   10714 non-null  int64  \n",
      " 20  isRomance       10714 non-null  int64  \n",
      " 21  isComedy        10714 non-null  int64  \n",
      " 22  isSport         10714 non-null  int64  \n",
      " 23  isWar           10714 non-null  int64  \n",
      " 24  isRated         10714 non-null  int64  \n",
      " 25  isFlop          10714 non-null  float64\n",
      "dtypes: float64(5), int64(21)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f2cb422-4b49-43ec-9dc0-7cf86974e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# lr = LinearRegression().fit(train_set, train_targets)\n",
    "# print(lr.predict(train_set) * 9 + 1)\n",
    "# print(train_targets * 9 + 1)\n",
    "# # print(lr.score(test_set, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93dfe7-8ce6-4048-8232-631eadd97b1f",
   "metadata": {},
   "source": [
    "### Logistic Regression, BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf82b0d-7478-4dcc-ae6b-ea1791b21320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "test_set = torch.from_numpy(test_set)\n",
    "test_set_normalized = (test_set - test_set.mean(dim=0, keepdims=True)) / test_set.std(dim=0, keepdims=True)\n",
    "test_set_normalized = torch.nan_to_num(test_set_normalized, nan=0)\n",
    "test_targets = torch.from_numpy(test_targets)\n",
    "\n",
    "train_set = torch.from_numpy(train_set)\n",
    "train_set_normalized = (train_set - train_set.mean(dim=0, keepdims=True)) / train_set.std(dim=0, keepdims=True)\n",
    "train_set_normalized = torch.nan_to_num(train_set_normalized, nan=0)\n",
    "train_targets = torch.from_numpy(train_targets)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(25, 1, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layer1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34129fcf-ad62-4c00-b730-995276203f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss (BCE) 0.6762, Validation Loss (BCE) 0.6582 Accuracy 0.597572386264801\n",
      "\n",
      "Epoch 200, Training Loss (BCE) 0.6674, Validation Loss (BCE) 0.6504 Accuracy 0.608776867389679\n",
      "\n",
      "Epoch 300, Training Loss (BCE) 0.6593, Validation Loss (BCE) 0.6433 Accuracy 0.6181139349937439\n",
      "\n",
      "Epoch 400, Training Loss (BCE) 0.6520, Validation Loss (BCE) 0.6369 Accuracy 0.6237161755561829\n",
      "\n",
      "Epoch 500, Training Loss (BCE) 0.6453, Validation Loss (BCE) 0.6310 Accuracy 0.6321195363998413\n",
      "\n",
      "Epoch 600, Training Loss (BCE) 0.6391, Validation Loss (BCE) 0.6256 Accuracy 0.6535947918891907\n",
      "\n",
      "Epoch 700, Training Loss (BCE) 0.6334, Validation Loss (BCE) 0.6206 Accuracy 0.6591970324516296\n",
      "\n",
      "Epoch 800, Training Loss (BCE) 0.6281, Validation Loss (BCE) 0.6160 Accuracy 0.6657329797744751\n",
      "\n",
      "Epoch 900, Training Loss (BCE) 0.6233, Validation Loss (BCE) 0.6118 Accuracy 0.6685341000556946\n",
      "\n",
      "Epoch 1000, Training Loss (BCE) 0.6188, Validation Loss (BCE) 0.6079 Accuracy 0.6676003932952881\n",
      "\n",
      "Epoch 1100, Training Loss (BCE) 0.6146, Validation Loss (BCE) 0.6043 Accuracy 0.676937460899353\n",
      "\n",
      "Epoch 1200, Training Loss (BCE) 0.6107, Validation Loss (BCE) 0.6009 Accuracy 0.680672287940979\n",
      "\n",
      "Epoch 1300, Training Loss (BCE) 0.6070, Validation Loss (BCE) 0.5977 Accuracy 0.6872082352638245\n",
      "\n",
      "Epoch 1400, Training Loss (BCE) 0.6036, Validation Loss (BCE) 0.5948 Accuracy 0.6918767690658569\n",
      "\n",
      "Epoch 1500, Training Loss (BCE) 0.6004, Validation Loss (BCE) 0.5921 Accuracy 0.6946778893470764\n",
      "\n",
      "Epoch 1600, Training Loss (BCE) 0.5974, Validation Loss (BCE) 0.5895 Accuracy 0.6965453028678894\n",
      "\n",
      "Epoch 1700, Training Loss (BCE) 0.5946, Validation Loss (BCE) 0.5871 Accuracy 0.6974790096282959\n",
      "\n",
      "Epoch 1800, Training Loss (BCE) 0.5919, Validation Loss (BCE) 0.5849 Accuracy 0.6984127163887024\n",
      "\n",
      "Epoch 1900, Training Loss (BCE) 0.5894, Validation Loss (BCE) 0.5828 Accuracy 0.7021475434303284\n",
      "\n",
      "Epoch 2000, Training Loss (BCE) 0.5871, Validation Loss (BCE) 0.5808 Accuracy 0.7058823704719543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model = Model()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    pred = model(train_set_normalized)\n",
    "    loss = loss_fn(pred.squeeze(), train_targets)\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss (BCE) {loss.item():.4f}\", end=\", \")\n",
    "        with torch.no_grad():\n",
    "            pred = model(test_set_normalized)\n",
    "            binary_pred = pred.round().squeeze()\n",
    "            acc = (binary_pred == test_targets).sum() / len(test_targets)\n",
    "            \n",
    "            val_loss = loss_fn(pred.squeeze(), test_targets)\n",
    "            print(f\"Validation Loss (BCE) {val_loss.item():.4f} Accuracy {acc.item()}\")\n",
    "        print()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250e540-3a80-4c06-b16d-ff5a52bc695c",
   "metadata": {},
   "source": [
    "### Large Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e1dd10-e19b-46fe-b093-39efb25f4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(25, 25, dtype=torch.double)\n",
    "        self.layer2 = nn.Linear(25, 25, dtype=torch.double)\n",
    "        self.layer3 = nn.Linear(25, 1, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return torch.sigmoid(self.layer3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30aa170e-d3b3-4651-a1a3-1864049dc924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training Loss (BCE) 0.7042, Validation Loss (BCE) 0.7107 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 200, Training Loss (BCE) 0.7031, Validation Loss (BCE) 0.7093 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 300, Training Loss (BCE) 0.7020, Validation Loss (BCE) 0.7081 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 400, Training Loss (BCE) 0.7010, Validation Loss (BCE) 0.7070 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 500, Training Loss (BCE) 0.7001, Validation Loss (BCE) 0.7059 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 600, Training Loss (BCE) 0.6992, Validation Loss (BCE) 0.7049 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 700, Training Loss (BCE) 0.6983, Validation Loss (BCE) 0.7040 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 800, Training Loss (BCE) 0.6976, Validation Loss (BCE) 0.7031 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 900, Training Loss (BCE) 0.6968, Validation Loss (BCE) 0.7023 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1000, Training Loss (BCE) 0.6961, Validation Loss (BCE) 0.7015 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1100, Training Loss (BCE) 0.6954, Validation Loss (BCE) 0.7007 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1200, Training Loss (BCE) 0.6948, Validation Loss (BCE) 0.7000 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1300, Training Loss (BCE) 0.6942, Validation Loss (BCE) 0.6993 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1400, Training Loss (BCE) 0.6936, Validation Loss (BCE) 0.6987 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1500, Training Loss (BCE) 0.6931, Validation Loss (BCE) 0.6981 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1600, Training Loss (BCE) 0.6925, Validation Loss (BCE) 0.6975 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1700, Training Loss (BCE) 0.6920, Validation Loss (BCE) 0.6969 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1800, Training Loss (BCE) 0.6915, Validation Loss (BCE) 0.6964 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 1900, Training Loss (BCE) 0.6910, Validation Loss (BCE) 0.6958 Accuracy 0.5004668831825256\n",
      "\n",
      "Epoch 2000, Training Loss (BCE) 0.6905, Validation Loss (BCE) 0.6953 Accuracy 0.5004668831825256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model = Model()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    pred = model(train_set_normalized)\n",
    "    loss = loss_fn(pred.squeeze(), train_targets)\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"Epoch {epoch + 1}, Training Loss (BCE) {loss.item():.4f}\", end=\", \")\n",
    "        with torch.no_grad():\n",
    "            pred = model(test_set_normalized)\n",
    "            binary_pred = pred.round().squeeze()\n",
    "            acc = (binary_pred == test_targets).sum() / len(test_targets)\n",
    "            \n",
    "            val_loss = loss_fn(pred.squeeze(), test_targets)\n",
    "            print(f\"Validation Loss (BCE) {val_loss.item():.4f} Accuracy {acc.item()}\")\n",
    "        print()\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25561717-6ddc-483b-8834-56def7e0640a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
